{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas, numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "allstar = pd.read_csv(\"./baseballdatabank-2019.2/core/AllstarFull.csv\")\n",
    "appear = pd.read_csv(\"./baseballdatabank-2019.2/core/Appearances.csv\")\n",
    "awardplay = pd.read_csv(\"./baseballdatabank-2019.2/core/AwardsPlayers.csv\")\n",
    "batting = pd.read_csv(\"./baseballdatabank-2019.2/core/Batting.csv\")\n",
    "battingpost = pd.read_csv(\"./baseballdatabank-2019.2/core/BattingPost.csv\")\n",
    "fielding = pd.read_csv(\"./baseballdatabank-2019.2/core/Fielding.csv\")\n",
    "\n",
    "#fieldingof = pd.read_csv(\"./baseballdatabank-2019.2/core/FieldingOF.csv\")\n",
    "\n",
    "fieldingofsplit = pd.read_csv(\"./baseballdatabank-2019.2/core/FieldingOFsplit.csv\")\n",
    "fieldingpost = pd.read_csv(\"./baseballdatabank-2019.2/core/FieldingPost.csv\")\n",
    "\n",
    "\n",
    "hof = pd.read_csv(\"./baseballdatabank-2019.2/core/HallOfFame.csv\")\n",
    "hof.drop(columns=['votedBy', 'ballots', 'needed', 'votes', 'needed_note'], axis=1, inplace=True)\n",
    "\n",
    "people = pd.read_csv(\"./baseballdatabank-2019.2/core/People.csv\")\n",
    "people.drop(columns = ['birthYear', 'birthMonth', 'birthDay', 'birthCountry', 'birthState', 'birthCity', 'deathYear', 'deathMonth', 'deathCountry', 'deathDay', 'deathState', 'deathCity', 'nameGiven'], axis=1, inplace=True)\n",
    "\n",
    "# pitch = pd.read_csv(\"./baseballdatabank-2019.2/core/Pitching.csv\")\n",
    "# pitchpost = pd.read_csv(\"./baseballdatabank-2019.2/core/PitchingPost.csv\")\n",
    "# salaries = pd.read_csv(\"./baseballdatabank-2019.2/core/Salaries.csv\")\n",
    "fangraph = pd.read_csv(\"./FanGraphs Leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop people who have not ended their careers or who never played a game\n",
    "\n",
    "debutnotnull = people[people['debut'].notnull()]\n",
    "finalgamenotnull = people[people['finalGame'].notnull()]\n",
    "people = pd.concat([debutnotnull,finalgamenotnull]).drop_duplicates()\n",
    "\n",
    "# make people debut and final game columns into dates\n",
    "people['debut'] = pd.to_datetime(people['debut'])\n",
    "people['finalGame'] = pd.to_datetime(people['finalGame'])\n",
    "\n",
    "# get difference between debuts and final games\n",
    "people['careerlength'] = people['finalGame'] - people['debut']\n",
    "# converts careerlength to # days\n",
    "people['careerlength'] = pd.to_timedelta(people['careerlength']).map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
    "\n",
    "people['recentretire'] = dt.datetime.now() - people['finalGame']\n",
    "people['recentretire'] = pd.to_timedelta(people['recentretire']).map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
    "\n",
    "# get careers > about 7 years\n",
    "people = people[people['careerlength'] > 2000]\n",
    "\n",
    "# remove those who retired < 5 years ago\n",
    "people = people[people['recentretire'] > 1825]\n",
    "\n",
    "people['Name'] = people['nameFirst'] + ' ' + people['nameLast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our learning, we are looking at all players who didn't make the Hall, not\n",
    "# just those who were voted on\n",
    "# so, we can remove those from the hall table that got nos, just leaving the yes\n",
    "hof = hof[hof['inducted'] == 'Y']\n",
    "\n",
    "# remove non-players from the hall\n",
    "hof = hof[hof['category'] == 'Player']\n",
    "\n",
    "# remove players from the hall that are not in our people table\n",
    "peopinhall = pd.merge(people, hof, on='playerID').drop_duplicates()['playerID']\n",
    "hof = hof[hof['playerID'].isin(peopinhall)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {'G' : 0, 'AB' : 0, 'R' : 0, 'H' : 0, '2B' : 0, '3B' : 0, 'HR' : 0, 'RBI' : 0, 'SB' : 0, 'CS' : 0, 'BB' : 0, 'SO' : 0, 'IBB' : 0, 'HBP' : 0, 'SH' : 0, 'SF' : 0, 'GIDP' : 0}\n",
    "\n",
    "# replace NaNs in batting with 0s\n",
    "batting.fillna(value=stats)\n",
    "battingpost.fillna(value=stats)\n",
    "\n",
    "# only get batting of people in our table\n",
    "batting = batting[batting['playerID'].isin(people['playerID'])]\n",
    "battingpost = battingpost[battingpost['playerID'].isin(people['playerID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "careerstats = batting.groupby('playerID').agg({'G' : np.sum, 'AB' : np.sum, 'R' : np.sum, 'H' : np.sum, '2B': np.sum, '3B' : np.sum, 'HR' : np.sum, 'RBI' : np.sum, 'SB' : np.sum, 'CS' : np.sum, 'BB' : np.sum, 'SO' : np.sum, 'IBB' : np.sum, 'HBP' : np.sum, 'SH' : np.sum, 'SF' : np.sum, 'GIDP' : np.sum})\n",
    "careerpoststats = battingpost.groupby('playerID').agg({'G' : np.sum, 'AB' : np.sum, 'R' : np.sum, 'H' : np.sum, '2B': np.sum, '3B' : np.sum, 'HR' : np.sum, 'RBI' : np.sum, 'SB' : np.sum, 'CS' : np.sum, 'BB' : np.sum, 'SO' : np.sum, 'IBB' : np.sum, 'HBP' : np.sum, 'SH' : np.sum, 'SF' : np.sum, 'GIDP' : np.sum})\n",
    "\n",
    "# filter out at-bats or games\n",
    "# filter out at-bats / game\n",
    "# or filter out games\n",
    "# 500 games seems like a good standard\n",
    "careerstats = careerstats[careerstats['G'] >= 500]\n",
    "careerstats['ABPG'] = careerstats['AB'] / careerstats['G']\n",
    "careerstats = careerstats[careerstats['ABPG'] >= 1] \n",
    "carrerstats = careerstats[careerstats['AB'] >= 1000]\n",
    "\n",
    "careerpoststats['ABPG'] = careerpoststats['AB'] / careerpoststats['G']\n",
    "careerpoststats = careerpoststats[careerpoststats['ABPG'] >= 1]\n",
    "\n",
    "# careerstats.count()\n",
    "# this leaves us with 2664 players, 212 of which are in the Hall of Fame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get appearances of only people in our table\n",
    "appear = appear[appear['playerID'].isin(people['playerID'])]\n",
    "\n",
    "stats = {'G_all' : 0, 'GS' : 0, 'G_batting' : 0, 'G_defense' : 0, 'G_p' : 0, 'G_c' : 0, 'G_1b' : 0, 'G_2b' : 0, 'G_3b' : 0, 'G_ss' : 0, 'G_lf' : 0, 'G_cf' : 0, 'G_rf' : 0, 'G_of' : 0, 'G_dh' : 0, 'G_ph' : 0, 'G_pr' : 0}\n",
    "appear.fillna(value=stats)\n",
    "\n",
    "careerapp = appear.groupby('playerID').agg({'G_all' : np.sum, 'G_batting' : np.sum, 'G_defense' : np.sum, 'G_p' : np.sum, 'G_c' : np.sum, 'G_1b' : np.sum, 'G_2b' : np.sum, 'G_3b' : np.sum, 'G_ss' : np.sum, 'G_lf' : np.sum, 'G_cf' : np.sum, 'G_rf' : np.sum, 'G_of' : np.sum, 'G_dh' : np.sum, 'G_ph' : np.sum, 'G_pr' : np.sum})\n",
    "\n",
    "careerapp = careerapp[careerapp['G_all'] >= 500]\n",
    "careerapp['pitchpct'] = careerapp['G_p'] / careerapp['G_all']\n",
    "careerapp = careerapp[careerapp['pitchpct'] < .75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fielding = fielding[fielding['playerID'].isin(people['playerID'])]\n",
    "stats = {'G' : 0, 'GS' : 0, 'InnOuts' : 0, 'PO' : 0, 'A' : 0, 'E' : 0, 'DP' : 0, 'PB' : 0, 'WP' : 0, 'SB' : 0, 'CS' : 0, 'ZR' : 0}\n",
    "fielding.fillna(value=stats)\n",
    "stats = {'G' : 0, 'GS' : 0, 'InnOuts' : 0, 'PO' : 0, 'A' : 0, 'E' : 0, 'DP' : 0, 'TP' : 0, 'PB' : 0, 'SB' : 0, 'CS' : 0}\n",
    "fieldingpost.fillna(value=stats)\n",
    "\n",
    "careerfield = fielding.groupby('playerID').agg({'G' : np.sum, 'GS' : np.sum, 'InnOuts' : np.sum, 'PO' : np.sum, 'A' : np.sum, 'E' : np.sum, 'DP' : np.sum, 'PB' : np.sum, 'WP' : np.sum, 'SB' : np.sum, 'CS' : np.sum, 'ZR' : np.sum})\n",
    "careerpostfield = fieldingpost.groupby('playerID').agg({'G' : np.sum, 'GS' : np.sum, 'InnOuts' : np.sum, 'PO' : np.sum, 'A' : np.sum, 'E' : np.sum, 'DP' : np.sum, 'PB' : np.sum, 'TP' : np.sum, 'SB' : np.sum, 'CS' : np.sum})\n",
    "\n",
    "careerfield = careerfield[careerfield['G'] >= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awards\n",
    "awardplay = awardplay[awardplay['playerID'].isin(people['playerID'])]\n",
    "awardplay['pitchtriplecrown'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Pitching Triple Crown' else 0)\n",
    "awardplay['triplecrown'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Triple Crown' else 0)\n",
    "awardplay['baseballmagallstar'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Baseball Magazine All-Star' else 0)\n",
    "awardplay['mvp'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Most Valuable Player' else 0)\n",
    "awardplay['tsnallstar'] = awardplay['awardID'].apply(lambda x : 1 if x == 'TSN All-Star' else 0)\n",
    "awardplay['tsnguidemvp'] = awardplay['awardID'].apply(lambda x : 1 if x == 'TSN Guide MVP' else 0)\n",
    "awardplay['tsnmlpoty'] = awardplay['awardID'].apply(lambda x : 1 if x == 'TSN Major League Player of the Year' else 0)\n",
    "awardplay['tsnpitcheroty'] = awardplay['awardID'].apply(lambda x : 1 if x == 'TSN Pitcher of the Year' else 0)\n",
    "awardplay['tsnpoty'] = awardplay['awardID'].apply(lambda x : 1 if x == 'TSN Player of the Year' else 0)\n",
    "awardplay['roty'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Rookie of the Year' else 0)\n",
    "awardplay['baberuth'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Babe Ruth Award' else 0)\n",
    "awardplay['lou'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Lou Gehrig Memorial Award' else 0)\n",
    "awardplay['wsmvp'] = awardplay['awardID'].apply(lambda x : 1 if x == 'World Series MVP' else 0)\n",
    "awardplay['cyyoung'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Cy Young Award' else 0)\n",
    "awardplay['goldglove'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Gold Glove' else 0)\n",
    "awardplay['fireman'] = awardplay['awardID'].apply(lambda x : 1 if x == 'TSN Fireman of the Year' else 0)\n",
    "awardplay['asgmvp'] = awardplay['awardID'].apply(lambda x : 1 if x == 'All-Star Game MVP' else 0)\n",
    "awardplay['hutch'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Hutch Award' else 0)\n",
    "awardplay['clemente'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Roberto Clemente Award' else 0)\n",
    "awardplay['relief'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Rolaids Relief Man Award' else 0)\n",
    "awardplay['nlcsmvp'] = awardplay['awardID'].apply(lambda x : 1 if x == 'NLCS MVP' else 0)\n",
    "awardplay['alcsmvp'] = awardplay['awardID'].apply(lambda x : 1 if x == 'ALCS MVP' else 0)\n",
    "awardplay['silver'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Silver Slugger' else 0)\n",
    "awardplay['branchrickey'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Branch Rickey Award' else 0)\n",
    "awardplay['hankaaron'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Hank Arron Award' else 0)\n",
    "awardplay['tsnrelief'] = awardplay['awardID'].apply(lambda x : 1 if x == 'TSN Reliever of the Year Award' else 0)\n",
    "awardplay['comeback'] = awardplay['awardID'].apply(lambda x : 1 if x == 'Comeback Player of the Year' else 0)\n",
    "\n",
    "careerawards = awardplay.groupby('playerID').agg({'pitchtriplecrown' : np.sum, 'triplecrown' : np.sum, 'baseballmagallstar' : np.sum, \n",
    "                                                 'mvp' : np.sum, 'tsnallstar' : np.sum, 'tsnguidemvp' : np.sum, 'tsnmlpoty' : np.sum,\n",
    "                                                 'tsnpitcheroty' : np.sum, 'tsnpoty' : np.sum, 'roty' : np.sum, 'baberuth' : np.sum,\n",
    "                                                 'lou' : np.sum, 'wsmvp' : np.sum, 'cyyoung' : np.sum, 'goldglove' : np.sum,\n",
    "                                                 'fireman' : np.sum, 'asgmvp' : np.sum, 'hutch' : np.sum, 'clemente' : np.sum,\n",
    "                                                 'relief' : np.sum, 'nlcsmvp' : np.sum, 'alcsmvp' : np.sum, 'silver' : np.sum,\n",
    "                                                 'branchrickey' : np.sum, 'hankaaron' : np.sum, 'tsnrelief' : np.sum,\n",
    "                                                 'comeback' : np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fanpeop = pd.merge(people, fangraph, on='Name')\n",
    "fanpeop = pd.merge(careerstats, fanpeop, on='playerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hofers = pd.merge(people, hof, on='playerID')\n",
    "hofbats = pd.merge(careerstats, hof, on='playerID')\n",
    "hofpost = pd.merge(careerpoststats, hof, on='playerID')\n",
    "hofapp = pd.merge(careerapp, hof, on='playerID')\n",
    "hoffield = pd.merge(careerfield, hof, on='playerID')\n",
    "hoffieldpost = pd.merge(careerpostfield, hof, on='playerID')\n",
    "hofawards = pd.merge(careerawards, hof, on='playerID')\n",
    "# hofers.count()\n",
    "# 229\n",
    "# hofbats.count()\n",
    "# 203\n",
    "# hofpost.count()\n",
    "# 189\n",
    "# hofapp.count()\n",
    "# 160\n",
    "# hoffield.count()\n",
    "# 209\n",
    "# hoffieldpost.count()\n",
    "# 182\n",
    "# hofawards.count()\n",
    "# 195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fanpeop.rename(index=str, columns={\"G_x\" : \"Career Stats G\", \"AB\" : \"Career Stats AB\", \"R_x\" : \"Career Stats R\", \"H\" : \"Career Stats H\",\n",
    "                                  \"2B\" : \"Career Stats 2B\", \"3B\" : \"Career Stats 3B\", \"HR_x\" : \"Career Stats HR\",\n",
    "                                  \"RBI_x\" : \"Career Stats RBI\", \"SB_x\" : \"Career Stats SB\", \"CS\" : \"Career Stats CS\",\n",
    "                                  \"BB\" : \"Career Stats BB\", \"SO\" : \"Career Stats SO\", \"IBB\" : \"Career Stats IBB\", \n",
    "                                  \"HBP\" : \"Career Stats HBP\", \"SH\" : \"Career Stats SH\", \"SF\": \"Career Stats SF\",\n",
    "                                  \"GIDP\" : \"Career Stats GIDP\", \"G_y\" : \"FanGraphs G\", \"PA\" : \"FanGraphs PA\",\n",
    "                                  \"HR_y\" : \"FanGraphs HR\", \"R_y\" : \"FanGraphs R\", \"RBI_y\" : \"FanGraphs RBI\",\n",
    "                                  \"SB_y\" : \"FanGraphs SB\"}, inplace=True)\n",
    "fanpeop.drop(columns=['nameFirst', 'nameLast', 'debut', 'finalGame', 'retroID', 'bbrefID', 'recentretire',\n",
    "                     'Name', 'Team', 'playerid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fanpeop\n",
    "data = pd.merge(data, careerawards, on='playerID', how='left').drop_duplicates()\n",
    "data.drop(columns=['ABPG', 'pitchtriplecrown', 'tsnpitcheroty', 'cyyoung', 'relief', 'tsnrelief'],axis=1, inplace=True)\n",
    "data.rename(index=str, columns={'pitchtriplecrown' : \"Pitching Triple Crown\", 'triplecrown' : \"Triple Crown\",\n",
    "                               'baseballmagallstar' : \"Baseball Mag All-Star\", 'mvp' : \"MVP\", 'tsnallstar' : \"TSN All-Star\",\n",
    "                               'tsnguidemvp' : \"TSN Guide MVP\", 'tsnmlpoty' : \"TSN ML POTY\", 'tsnpoty' : \"TSN POTY\",\n",
    "                               'roty' : \"Rookie of the Year\", 'baberuth' : \"Babe Ruth Award\", 'lou' : \"Lou Gehrig Memorial Award\",\n",
    "                               'wsmvp' : \"World Series MVP\", 'goldglove' : \"Gold Glove\", 'fireman' : \"TSN Fireman of the Year\",\n",
    "                               'asgmvp' : \"ASG MVP\", 'hutch' : \"Hutch Award\", 'clemente' : \"Roberto Clemente Award\",\n",
    "                               'nlcsmvp' : \"NLCS MVP\", 'alcsmvp' : \"ALCS MVP\", 'silver' : \"Silver Slugger\", \n",
    "                               'branchrickey' : \"Branch Rickey Award\", 'hankaaron' : \"Hank Aaron Award\", \n",
    "                               'comeback' : \"Comeback Player of the Year\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, careerfield, on='playerID', how='left').drop_duplicates()\n",
    "data.drop(columns=['WP', 'SB', 'CS', 'ZR', 'PB'],axis=1, inplace=True)\n",
    "data.rename(index=str, columns={'G' : 'Games Fielding', 'GS' : 'GS Fielding', 'InnOuts' : 'Inning Outs Fielding',\n",
    "                               'PO' : 'PO Fielding', 'A' : 'A Fielding', 'E' : 'E Fielding',\n",
    "                               'DP' : 'DP Fielding'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, careerpostfield, on='playerID', how='left').drop_duplicates()\n",
    "data.drop(columns=['PB', 'TP', 'CS', 'SB'], axis=1, inplace=True)\n",
    "data.rename(index=str, columns={'G' : 'G Fielding Post', 'GS' : 'GS Fielding Post', 'InnOuts' : 'Inning Outs Fielding Post', \n",
    "                               'PO' : 'PO Fielding Post', 'A' : 'A Fielding Post', 'E' : 'E Fielding Post',\n",
    "                               'DP' : 'DP Fielding Post'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, careerpoststats, on='playerID', how='left').drop_duplicates()\n",
    "data.drop(columns=['ABPG'], axis=1,inplace=True)\n",
    "data.rename(index=str, columns={'G' : 'G Post', 'AB' : 'AB Post', 'R' : 'R Post', 'H' : 'H Post', '2B' : '2B Post',\n",
    "                               '3B' : '3B Post', 'HR' : 'HR Post', 'RBI' : 'RBI Post' ,'SB' : 'SB Post', 'CS' : 'CS Post',\n",
    "                               'BB' : 'BB Post', 'SO' : 'SO Post', 'IBB' : 'IBB Post', 'HBP' : 'HBP Post', \n",
    "                               'SH' : 'SH Post', 'SF' : 'SF Post', 'GIDP' : 'GIDP Post'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data = data.fillna(0)\n",
    "data['Bats'] = data['bats'].apply(lambda x : 1 if x == 'R' else 0)\n",
    "data['Throws'] = data['throws'].apply(lambda x : 1 if x == 'R' else 0)\n",
    "data.drop(columns=['bats', 'throws'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['K%'] = data['K%'].astype(str)\n",
    "data['K%'] = data['K%'].str.replace(' %$', '')\n",
    "data['BB%'] = data['BB%'].astype(str)\n",
    "data['BB%'] = data['BB%'].str.replace(' %$', '')\n",
    "data = data.replace('nan', 50)\n",
    "data[['K%', 'BB%']] = data[['K%', 'BB%']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist = data.columns.tolist()\n",
    "data[collist[1:]] = data[collist[1:]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoferslist = hofers.columns.tolist()\n",
    "hofers = hofers[hoferslist[0]]\n",
    "hoflist = hofers.tolist()\n",
    "data['HOF'] = data['playerID'].apply(lambda x : 1 if x in hoflist else 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hoflessgames = data[(data['Career Stats G'] < 1000) & (data['HOF'] == 1)]\n",
    "data = data[data['Career Stats G'] >= 1000]\n",
    "data = data[data['Career Stats H'] >= 500]\n",
    "\n",
    "# data = pd.concat([data, hoflessgames]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(data.iloc[:,[-1]])\n",
    "labels = np.squeeze(np.asarray(labels))\n",
    "data.drop(columns=['HOF', 'FanGraphs G', 'FanGraphs PA', 'FanGraphs R', 'FanGraphs RBI', 'FanGraphs SB', 'Baseball Mag All-Star', 'TSN All-Star',\n",
    "                  'careerlength'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "#TODO: make sure that we have the correct thresholds for stats we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "data_frame = data[data.columns[1:]]\n",
    "data_new = SelectKBest(mutual_info_classif, k=25).fit_transform(data_frame, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data[data.columns[1:]]\n",
    "\n",
    "selector = SelectKBest(mutual_info_classif, k=25)\n",
    "selector.fit(data_frame, labels)\n",
    "# Get columns to keep\n",
    "cols = selector.get_support(indices=True)\n",
    "# Create new dataframe with only desired columns, or overwrite existing\n",
    "data_reduct = data_frame.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "data_scale = preprocessing.scale(data_reduct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.asarray(np.where(np.isin(labels, 1)))\n",
    "ix = ix[0]\n",
    "hof_reduct = data_reduct.iloc[ix,:]\n",
    "ixnot = np.asarray(np.where(np.isin(labels,0)))\n",
    "ixnot = ixnot[0]\n",
    "nothof_reduct = data_reduct.iloc[ixnot,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Career Stats GIDP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Career Stats GIDP'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-30494efc717d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0max11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HoF Careers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0max12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhof_reduct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Career Stats GIDP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0max12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distribution of GIDP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0max12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HoF Careers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Career Stats GIDP'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the figure and add subplots\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax1 = fig.add_subplot(5,5,1)\n",
    "ax2 = fig.add_subplot(5,5,2)\n",
    "ax3 = fig.add_subplot(5,5,3)\n",
    "ax4 = fig.add_subplot(5,5,4)\n",
    "ax5 = fig.add_subplot(5,5,5)\n",
    "ax6 = fig.add_subplot(5,5,6)\n",
    "ax7 = fig.add_subplot(5,5,7)\n",
    "ax8 = fig.add_subplot(5,5,8)\n",
    "ax9 = fig.add_subplot(5,5,9)\n",
    "ax10 = fig.add_subplot(5,5,10)\n",
    "ax11 = fig.add_subplot(5,5,11)\n",
    "ax12 = fig.add_subplot(5,5,12)\n",
    "ax13 = fig.add_subplot(5,5,13)\n",
    "ax14 = fig.add_subplot(5,5,14)\n",
    "ax15 = fig.add_subplot(5,5,15)\n",
    "ax16 = fig.add_subplot(5,5,16)\n",
    "ax17 = fig.add_subplot(5,5,17)\n",
    "ax18 = fig.add_subplot(5,5,18)\n",
    "ax19 = fig.add_subplot(5,5,19)\n",
    "ax20 = fig.add_subplot(5,5,20)\n",
    "ax21 = fig.add_subplot(5,5,21)\n",
    "ax22 = fig.add_subplot(5,5,22)\n",
    "ax23 = fig.add_subplot(5,5,23)\n",
    "ax24 = fig.add_subplot(5,5,24)\n",
    "ax25 = fig.add_subplot(5,5,25)\n",
    "\n",
    "# Create distribution plots for Hits, Home Runs, Years Played and All Star Games\n",
    "ax1.hist(hof_reduct['Career Stats G'])\n",
    "ax1.set_title('Distribution of Games')\n",
    "ax1.set_ylabel('HoF Careers')\n",
    "ax2.hist(hof_reduct['Career Stats AB'])\n",
    "ax2.set_title('Distribution of AB')\n",
    "ax3.hist(hof_reduct['Career Stats R'])\n",
    "ax3.set_title('Distribution of Runs')\n",
    "ax3.set_ylabel('HoF Careers')\n",
    "ax4.hist(hof_reduct['Career Stats H'])\n",
    "ax4.set_title('Distribution of Hits')\n",
    "ax5.hist(hof_reduct['Career Stats 2B'])\n",
    "ax5.set_title('Distribution of Doubles')\n",
    "ax5.set_ylabel('HoF Careers')\n",
    "\n",
    "ax6.hist(hof_reduct['Career Stats 3B'])\n",
    "ax6.set_title('Distribution of Triples')\n",
    "ax6.set_ylabel('HoF Careers')\n",
    "\n",
    "ax7.hist(hof_reduct['Career Stats RBI'])\n",
    "ax7.set_title('Distribution of RBI')\n",
    "ax7.set_ylabel('HoF Careers')\n",
    "\n",
    "ax8.hist(hof_reduct['A Fielding'])\n",
    "ax8.set_title('Distribution of Assists Fielding')\n",
    "ax8.set_ylabel('HoF Careers')\n",
    "\n",
    "ax9.hist(hof_reduct['Career Stats BB'])\n",
    "ax9.set_title('Distribution of Walks')\n",
    "ax9.set_ylabel('HoF Careers')\n",
    "\n",
    "ax10.hist(hof_reduct['Career Stats IBB'])\n",
    "ax10.set_title('Distribution of Intentional Walks')\n",
    "ax10.set_ylabel('HoF Careers')\n",
    "\n",
    "ax11.hist(hof_reduct['Career Stats SF'])\n",
    "ax11.set_title('Distribution of Sacrifice Flies')\n",
    "ax11.set_ylabel('HoF Careers')\n",
    "\n",
    "ax12.hist(hof_reduct['Career Stats GIDP'])\n",
    "ax12.set_title('Distribution of GIDP')\n",
    "ax12.set_ylabel('HoF Careers')\n",
    "\n",
    "#ax13.hist(hof_reduct['BABIP'])\n",
    "#ax13.set_title('Distribution of BABIP')\n",
    "#ax13.set_ylabel('HoF Careers')\n",
    "\n",
    "ax14.hist(hof_reduct['AVG'])\n",
    "ax14.set_title('Distribution of AVG')\n",
    "ax14.set_ylabel('HoF Careers')\n",
    "\n",
    "ax15.hist(hof_reduct['OBP'])\n",
    "ax15.set_title('Distribution of OBP')\n",
    "ax15.set_ylabel('HoF Careers')\n",
    "\n",
    "ax16.hist(hof_reduct['SLG'])\n",
    "ax16.set_title('Distribution of SLG')\n",
    "ax16.set_ylabel('HoF Careers')\n",
    "\n",
    "ax17.hist(hof_reduct['wOBA'])\n",
    "ax17.set_title('Distribution of wOBA')\n",
    "ax17.set_ylabel('HoF Careers')\n",
    "\n",
    "ax18.hist(hof_reduct['wRC+'])\n",
    "ax18.set_title('Distribution of wRC+')\n",
    "ax18.set_ylabel('HoF Careers')\n",
    "\n",
    "ax19.hist(hof_reduct['Off'])\n",
    "ax19.set_title('Distribution of Off')\n",
    "ax19.set_ylabel('HoF Careers')\n",
    "\n",
    "ax20.hist(hof_reduct['WAR'])\n",
    "ax20.set_title('Distribution of WAR')\n",
    "ax20.set_ylabel('HoF Careers')\n",
    "\n",
    "ax21.hist(hof_reduct['Games Fielding'])\n",
    "ax21.set_title('Distribution of Games Fielding')\n",
    "ax21.set_ylabel('HoF Careers')\n",
    "\n",
    "ax22.hist(hof_reduct['GS Fielding'])\n",
    "ax22.set_title('Distribution of Games Started Fielding')\n",
    "ax22.set_ylabel('HoF Careers')\n",
    "\n",
    "ax23.hist(hof_reduct['Inning Outs Fielding'])\n",
    "ax23.set_title('Distribution of Inning Outs Fielding')\n",
    "ax23.set_ylabel('HoF Careers')\n",
    "\n",
    "ax24.hist(hof_reduct['PO Fielding'])\n",
    "ax24.set_title('Distribution of PO Fielding')\n",
    "ax24.set_ylabel('HoF Careers')\n",
    "\n",
    "ax25.hist(hof_reduct['MVP'])\n",
    "ax25.set_title('Distribution of MVP')\n",
    "ax25.set_ylabel('HoF Careers')\n",
    "\n",
    "stats = ['wOBA', 'wRC+', 'Off', 'WAR', 'Games Fielding', 'GS Fielding',\n",
    "       'Inning Outs Fielding', 'PO Fielding', 'A Fielding']\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure and add subplots\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "# Create Scatter plots for Hits vs. Average and Home Runs vs. Average\n",
    "ax1.scatter(hof_reduct['Career Stats H'], hof_reduct['Career Stats G'], c='r', label='HoF Player')\n",
    "ax1.scatter(nothof_reduct['Career Stats H'], nothof_reduct['Career Stats G'], c='b', label='Non HoF Player')\n",
    "ax1.set_title('Career Hits vs. Career Games')\n",
    "ax1.set_xlabel('Career Hits')\n",
    "ax1.set_ylabel('Career Games')\n",
    "ax2.scatter(hof_reduct['Career Stats H'], hof_reduct['WAR'], c='r', label='HoF Player')\n",
    "ax2.scatter(nothof_reduct['Career Stats H'], nothof_reduct['WAR'], c='b', label='Non HoF Player')\n",
    "ax2.set_title('Career Hits vs. Career WAR')\n",
    "ax2.set_ylabel('Career WAR')\n",
    "ax2.set_xlabel('Career Hits')\n",
    "ax2.legend(loc='lower right', scatterpoints=1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Begin Clustering**\n",
    "\n",
    "My goal with the clustering is to find groups of hall of fame players who are similar to answer the question \"what types of players get into the hall of fame?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for clustering\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a variety of models with varying numbers of clusters\n",
    "for num_clusters in range(2,30):\n",
    "    km_model = KMeans(n_clusters=num_clusters, max_iter=500)\n",
    "    km_model.fit(hof_reduct)\n",
    "    ac_model = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "    ac_model.fit(hof_reduct)\n",
    "    gmf_model = GaussianMixture(n_components=num_clusters, covariance_type=\"full\", n_init=10)\n",
    "    gmf_model.fit(hof_reduct)\n",
    "    gms_model = GaussianMixture(n_components=num_clusters, covariance_type=\"spherical\", n_init=10)\n",
    "    gms_model.fit(hof_reduct)\n",
    "    gmd_model = GaussianMixture(n_components=num_clusters, covariance_type=\"diag\", n_init=10)\n",
    "    gmd_model.fit(hof_reduct)\n",
    "    gmt_model = GaussianMixture(n_components=num_clusters, covariance_type=\"tied\", n_init=10)\n",
    "    gmt_model.fit(hof_reduct)\n",
    "    \n",
    "    print(\"Training models with \" + str(num_clusters) + \" clusters\")\n",
    "    print(\"    {:<65} {:>10f}\".format(\"k-means silhouette score:\", metrics.silhouette_score(hof_reduct, km_model.labels_)))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"agglomerative clustering silhouette score:\", metrics.silhouette_score(hof_reduct, ac_model.labels_)))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"gaussian mixtures with full covariance silhouette score:\", metrics.silhouette_score(hof_reduct, gmf_model.predict(hof_reduct))))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"gaussian mixtures with diagonal covariance silhouette score:\", metrics.silhouette_score(hof_reduct, gmd_model.predict(hof_reduct))))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"gaussian mixtures with spherical covariance silhouette score:\", metrics.silhouette_score(hof_reduct, gms_model.predict(hof_reduct))))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"gaussian mixtures with tied covariance silhouette score:\", metrics.silhouette_score(hof_reduct, gmt_model.predict(hof_reduct))))\n",
    "    print(\"\\n\")\n",
    "    print(\"    {:<65} {:>10f}\".format(\"v measure agreement score, k-means and spherical:\", metrics.v_measure_score(km_model.labels_, gms_model.predict(hof_reduct))))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"v measure agreement score, k-means and agglomerative:\", metrics.v_measure_score(km_model.labels_, ac_model.labels_)))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"v measure agreement score, k-means and full:\", metrics.v_measure_score(km_model.labels_, gmf_model.predict(hof_reduct))))\n",
    "    print(\"    {:<65} {:>10f}\".format(\"v measure agreement score, agglomerative and full:\", metrics.v_measure_score(ac_model.labels_, gmf_model.predict(hof_reduct))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DBSCAN model\n",
    "scaled_data = scale(hof_reduct)\n",
    "best_sil_score = 0\n",
    "best_ep = 0\n",
    "best_min = 0\n",
    "for epsilon in range(24,36):\n",
    "    epsilon = float(epsilon)/6\n",
    "    for min_cluster_size in range(3,7):\n",
    "        db_model = DBSCAN(eps=float(epsilon), min_samples=min_cluster_size)\n",
    "        db_model.fit(scaled_data)\n",
    "        cluster_count = len(set(db_model.labels_)) - (1 if -1 in db_model.labels_ else 0)\n",
    "        n_noise_points = list(db_model.labels_).count(-1)\n",
    "        print(\"Training DBSCAN model with epsilon value {} and minimum cluster size {}\".format(epsilon, min_cluster_size))\n",
    "        print(\"    {:<50} {:>10f}\".format(\"Number of clusters:\", cluster_count))\n",
    "        print(\"    {:<50} {:>10f}\".format(\"Number of samples classified as noise:\", n_noise_points))\n",
    "        if len(set(db_model.labels_)) > 1:\n",
    "            sil_score = metrics.silhouette_score(scaled_data, db_model.labels_)\n",
    "            print(\"    {:<50} {:>10f}\".format(\"Silhouette score:\", sil_score))\n",
    "            if sil_score > best_sil_score:\n",
    "                best_sil_score = sil_score\n",
    "                best_ep = float(epsilon)\n",
    "                best_min = min_cluster_size\n",
    "print(\"\\nThe best parameters were epsilon={} and min_samples={} with a silhouette score of {}\".format(best_ep, best_min, best_sil_score))\n",
    "\n",
    "#TODO: Determine who got classified as noise (especially in the best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit affinity propogation model\n",
    "ap_model = AffinityPropagation()\n",
    "ap_model.fit(hof_reduct)\n",
    "print(\"Number of clusters in affinity propagation model: {}\".format(len(ap_model.cluster_centers_indices_)))\n",
    "print(\"Silhouette score of affinity propagation model: {}\".format(metrics.silhouette_score(hof_reduct, ap_model.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the model we will investigate more\n",
    "model = KMeans(n_clusters=2, max_iter=500)\n",
    "labels = model.fit_predict(hof_reduct)\n",
    "\n",
    "# Sanity checking\n",
    "print(\"type of hof_reduct: {}\".format(type(hof_reduct)))\n",
    "print(\"shape of data: {}\".format(hof_reduct.shape))\n",
    "print(\"shape of labels: {}\".format(labels.shape))\n",
    "zero_bools = [True if i == 1 else False for i in labels]\n",
    "one_bools = [True if i == 0 else False for i in labels]\n",
    "\n",
    "cluster_zero_data = hof_reduct[zero_bools]\n",
    "cluster_one_data = hof_reduct[one_bools]\n",
    "\n",
    "print(\"shape of cluster 0 data: {}\".format(cluster_zero_data.shape))\n",
    "print(\"shape of cluster 1 data: {}\".format(cluster_one_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell recreates the plots from above but only for one cluster at a time\n",
    "# It displays one stat per row and places the two clusters side-by-side\n",
    "\n",
    "fig = plt.figure(figsize=(20, 140))\n",
    "\n",
    "stat_strings = list(hof_reduct)\n",
    "num_stats = len(stat_strings)\n",
    "\n",
    "subplots=[None]\n",
    "for i in range(1, (2*num_stats)+1):\n",
    "    subplots.append(fig.add_subplot(num_stats,2,i))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_stats):\n",
    "    index = 2*(i+1)\n",
    "    subplots[index].hist(cluster_one_data[stat_strings[i]])\n",
    "    subplots[index].set_title(stat_strings[i])\n",
    "    subplots[index].set_ylabel('Cluster One')\n",
    "    index = index - 1\n",
    "    subplots[index].hist(cluster_zero_data[stat_strings[i]])\n",
    "    subplots[index].set_title(stat_strings[i])\n",
    "    subplots[index].set_ylabel('Cluster Zero')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some quick conclusions**\n",
    "\n",
    "The players in Cluster One seem to be a little better in almost every stat. My theory is that Cluster Zero is older players and Cluster One is newer players. That would make sense if it has gotten harder to get into the hall of fame, but I don't know if that's true. That would also make sense for some of the newer statistics. For example, it probably didn't matter what your wOBA was in 1950 because Hall of Fame voters weren't tracking that. So you could have gotten into the Hall of Fame with worse statistics in previous years because no one knew they were bad. But we should definitely find out if the two clusters are split up by time before we come up with any more theories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
